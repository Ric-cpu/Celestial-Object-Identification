{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32775a96",
   "metadata": {},
   "source": [
    "##  EXPERIMENT USING STARS, EXOPLANETS AND QUASARS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aea39d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b10e3c",
   "metadata": {},
   "source": [
    "## checking for missing/null values (extra caution before starting the new experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "718085e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_id          0\n",
      "ra                 0\n",
      "dec                0\n",
      "parallax           0\n",
      "pmra               0\n",
      "pmdec              0\n",
      "phot_g_mean_mag    0\n",
      "bp_rp              0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   source_id        50000 non-null  int64  \n",
      " 1   ra               50000 non-null  float64\n",
      " 2   dec              50000 non-null  float64\n",
      " 3   parallax         50000 non-null  float64\n",
      " 4   pmra             50000 non-null  float64\n",
      " 5   pmdec            50000 non-null  float64\n",
      " 6   phot_g_mean_mag  50000 non-null  float64\n",
      " 7   bp_rp            50000 non-null  float64\n",
      "dtypes: float64(7), int64(1)\n",
      "memory usage: 3.1 MB\n",
      "None\n",
      "rowid              0\n",
      "pl_name            0\n",
      "hostname           0\n",
      "pl_letter          0\n",
      "gaia_id            0\n",
      "discoverymethod    0\n",
      "disc_year          0\n",
      "pl_orbper          0\n",
      "pl_orbsmax         0\n",
      "st_teff            0\n",
      "st_rad             0\n",
      "st_mass            0\n",
      "ra                 0\n",
      "dec                0\n",
      "sy_dist            0\n",
      "sy_plx             0\n",
      "pmra               0\n",
      "pmdec              0\n",
      "phot_g_mean_mag    0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37871 entries, 0 to 37870\n",
      "Data columns (total 19 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   rowid            37871 non-null  int64  \n",
      " 1   pl_name          37871 non-null  object \n",
      " 2   hostname         37871 non-null  object \n",
      " 3   pl_letter        37871 non-null  object \n",
      " 4   gaia_id          37871 non-null  object \n",
      " 5   discoverymethod  37871 non-null  object \n",
      " 6   disc_year        37871 non-null  int64  \n",
      " 7   pl_orbper        37871 non-null  float64\n",
      " 8   pl_orbsmax       37871 non-null  float64\n",
      " 9   st_teff          37871 non-null  float64\n",
      " 10  st_rad           37871 non-null  float64\n",
      " 11  st_mass          37871 non-null  float64\n",
      " 12  ra               37871 non-null  float64\n",
      " 13  dec              37871 non-null  float64\n",
      " 14  sy_dist          37871 non-null  float64\n",
      " 15  sy_plx           37871 non-null  float64\n",
      " 16  pmra             37871 non-null  float64\n",
      " 17  pmdec            37871 non-null  float64\n",
      " 18  phot_g_mean_mag  37871 non-null  float64\n",
      "dtypes: float64(12), int64(2), object(5)\n",
      "memory usage: 5.5+ MB\n",
      "None\n",
      "source_id             0\n",
      "ra                    0\n",
      "dec                   0\n",
      "parallax              0\n",
      "pmra                  0\n",
      "pmdec                 0\n",
      "phot_g_mean_mag       0\n",
      "bp_rp                 0\n",
      "ruwe                  0\n",
      "quasar_probability    0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   source_id           50000 non-null  int64  \n",
      " 1   ra                  50000 non-null  float64\n",
      " 2   dec                 50000 non-null  float64\n",
      " 3   parallax            50000 non-null  float64\n",
      " 4   pmra                50000 non-null  float64\n",
      " 5   pmdec               50000 non-null  float64\n",
      " 6   phot_g_mean_mag     50000 non-null  float64\n",
      " 7   bp_rp               50000 non-null  float64\n",
      " 8   ruwe                50000 non-null  float64\n",
      " 9   quasar_probability  50000 non-null  float64\n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 3.8 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# dealing with any missing/null values for stars, exopalnets and quasars datasets \n",
    "stars_df = pd.read_csv(\"../../data/full/stars_gaia_clean.csv\")\n",
    "print(stars_df.isnull().sum())\n",
    "print(stars_df.info())\n",
    "\n",
    "exo_df = pd.read_csv(\"../../data/full/exoplanets_clean.csv\")\n",
    "print(exo_df.isnull().sum())\n",
    "print(exo_df.info())\n",
    "\n",
    "quasars_df = pd.read_csv(\"../../data/full/quasars_gaia_clean.csv\")\n",
    "print(quasars_df.isnull().sum())\n",
    "print(quasars_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b376763",
   "metadata": {},
   "source": [
    "## Loading Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b722af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laad datasets\n",
    "stars_df = pd.read_csv(\"../../data/full/stars_gaia_clean.csv\").assign(label=\"star\")\n",
    "exo_df = pd.read_csv(\"../../data/full/exoplanets_clean.csv\").assign(label=\"exo_host\")\n",
    "quasars_df = pd.read_csv(\"../../data/full/quasars_gaia_clean.csv\").assign(label=\"quasar\")\n",
    "\n",
    "\n",
    "#align column: exo use sy_plx while stars and qiuasars use parallax\n",
    "exo_df = exo_df.rename(columns={\"sy_plx\":\"parallax\", \"sy_pmra\":\"pmra\", \"sy_pmdec\":\"pmdec\", \"sy_gaiamag\":\"phot_g_mean_mag\"})\n",
    "\n",
    "\n",
    "# select shared features\n",
    "features = [\"ra\", \"dec\", \"parallax\", \"pmdec\", \"pmra\", \"phot_g_mean_mag\"]\n",
    "\n",
    "# Define dataset size for each datasets \n",
    "sizes = [10000, 20000, 37800]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a78fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "exo_df = exo_df.dropna(subset=features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21675a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Trim datasets to only have selected features and drop rows with missing values\n",
    "stars_df = stars_df[features + [\"label\"]].dropna()\n",
    "exo_df = exo_df[features +[\"label\"]].dropna()\n",
    "quasars_df = quasars_df[features + [\"label\"]].dropna()\n",
    "\n",
    "\n",
    "\n",
    "# This satcks the three datasets toghether, so i can threat them as a single multiclass dataset\n",
    "df = pd.concat([stars_df, exo_df, quasars_df])\n",
    "\n",
    "# Split in X (Numeric Feature only) and y (Call labels (Star, Exoplanet, Quasar)\n",
    "\n",
    "X = df[features]\n",
    "y = df[\"label\"]\n",
    "\n",
    "\n",
    "# labeled-feature datasets only \n",
    "subsets = {}\n",
    "\n",
    "## Creates a balanced subsets for each dataset size\n",
    "for n_total in sizes:\n",
    "    n_per = min(n_total // 3, len(stars_df), len(exo_df), len(quasars_df))\n",
    "\n",
    "    \n",
    "    stars_sub = stars_df.sample(n=n_per, random_state=42)\n",
    "    exo_sub = exo_df.sample(n=n_per, random_state=42)\n",
    "    quasars_sub = quasars_df.sample(n=n_per, random_state=42)\n",
    "\n",
    "    df_sub = pd.concat([stars_sub, exo_sub, quasars_sub], ignore_index=True)\n",
    "    X_n = df_sub[features]\n",
    "    y_n = df_sub[\"label\"]\n",
    "    subsets[n_total] = (X_n, y_n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572342a4",
   "metadata": {},
   "source": [
    "## Baselien sanity (DummyClassifier)\n",
    "### I have 3 kinds of objects: Stars, Exoplanets, Quasars. \n",
    "### See if the model does better than random guessing, so i make a \"dummy\" model that just guesses in a simple way, and check how bad it is.\n",
    "### If tha main model beats this dummy, i know im doing real learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "824d5adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ra', 'dec', 'parallax', 'pmdec', 'pmra', 'phot_g_mean_mag', 'label'], dtype='object')\n",
      "ra                 0.0\n",
      "dec                0.0\n",
      "parallax           0.0\n",
      "pmdec              0.0\n",
      "pmra               0.0\n",
      "phot_g_mean_mag    0.0\n",
      "dtype: float64\n",
      "exo rows before dropna: 37871\n"
     ]
    }
   ],
   "source": [
    "print(exo_df.columns)\n",
    "print(exo_df[features].isna().mean().sort_values(ascending=False))\n",
    "print(\"exo rows before dropna:\", len(exo_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5adf047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy most_frequent: 0.33333333333333337 0.16666666666666669\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "X_n, y_n = subsets[10000]\n",
    "\n",
    "for n in sizes:\n",
    "    X_n, y_n = subsets[n]\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "\n",
    "accs, f1s = [], []\n",
    "for train_idx, test_idx in splitter.split(X_n, y_n):\n",
    "    clf = DummyClassifier(strategy=\"most_frequent\", random_state=42)\n",
    "    clf.fit(X_n.iloc[train_idx], y_n.iloc[train_idx])\n",
    "    pred = clf.predict(X_n.iloc[test_idx])\n",
    "    accs.append(accuracy_score(y_n.iloc[test_idx], pred))\n",
    "    f1s.append(f1_score(y_n.iloc[test_idx], pred, average=\"macro\"))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Dummy most_frequent:\", np.mean(accs), np.mean(f1s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa68ec4",
   "metadata": {},
   "source": [
    "## Logistic regression, Random Forest and Multi-layer perception (Main supervised models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bedb052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'size': 9999, 'acc_mean': np.float64(0.9774833333333335), 'f1_mean': np.float64(0.9774779968974233), 'prec_mean': np.float64(0.9775730698823865), 'rec_mean': np.float64(0.9774813544178861), 'train_acc_mean': np.float64(0.9774805183981332), 'test_acc_mean': np.float64(0.9774833333333335), 'acc_gap': np.float64(-2.8149352002682093e-06)}\n",
      "{'size': 19998, 'acc_mean': np.float64(0.9794500000000002), 'f1_mean': np.float64(0.979446226921007), 'prec_mean': np.float64(0.9794854066130139), 'rec_mean': np.float64(0.9794508159773578), 'train_acc_mean': np.float64(0.9794390965537357), 'test_acc_mean': np.float64(0.9794500000000002), 'acc_gap': np.float64(-1.0903446264420857e-05)}\n",
      "{'size': 37800, 'acc_mean': np.float64(0.9796869488536154), 'f1_mean': np.float64(0.9796832753779882), 'prec_mean': np.float64(0.9797240999725578), 'rec_mean': np.float64(0.9796869488536156), 'train_acc_mean': np.float64(0.9797001763668434), 'test_acc_mean': np.float64(0.9796869488536154), 'acc_gap': np.float64(1.3227513227964671e-05)}\n"
     ]
    }
   ],
   "source": [
    "## Main supervised models (LogReg, RF, MPL)\n",
    "\n",
    "\n",
    "#align column: exo use sy_plx while stars and qiuasars use parallax\n",
    "exo_df = exo_df.rename(columns={\"sy_plx\":\"parallax\", \"sy_pmra\":\"pmra\", \"sy_pmdec\":\"pmdec\", \"sy_gaiamag\":\"phot_g_mean_mag\"})\n",
    "\n",
    "\n",
    "# select shared features\n",
    "features = [\"ra\", \"dec\", \"parallax\", \"pmdec\", \"pmra\", \"phot_g_mean_mag\"]\n",
    "\n",
    "# Define dataset size for each datasets \n",
    "sizes = [10000, 20000, 37800]\n",
    "\n",
    "# Trim datasets to only have selected features and drop rows with missing values\n",
    "stars = stars_df[features + [\"label\"]].dropna()\n",
    "exo = exo_df[features +[\"label\"]].dropna()\n",
    "quasars = quasars_df[features + [\"label\"]].dropna()\n",
    "\n",
    "def eval_size(n_total, n_splits=30):\n",
    "    n_per = min(n_total // 3, len(exo), len(stars), len(quasars))\n",
    "    stars_sub = stars.sample(n=n_per, random_state=42)\n",
    "    exo_sub = exo.sample(n=n_per, random_state=42)\n",
    "    quasars_sub = quasars.sample(n=n_per, random_state=42)\n",
    "\n",
    "    df = pd.concat([exo_sub, stars_sub, quasars_sub], ignore_index=True)\n",
    "    X = df[features].values\n",
    "    y = df[\"label\"].values\n",
    "\n",
    "    splitter = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.2, random_state=42)\n",
    "    accs, f1s, precs, recs = [], [], [], []\n",
    "    train_accs, test_accs = [], []\n",
    "\n",
    "    clf = make_pipeline(StandardScaler(), LogisticRegression(max_iter=2000))\n",
    "    for train_idx, test_idx in splitter.split(X, y):\n",
    "        clf.fit(X[train_idx], y[train_idx])\n",
    "        pred = clf.predict(X[test_idx])\n",
    "        p, r, f1, _ = precision_recall_fscore_support(\n",
    "        y[test_idx], pred, average=\"macro\", zero_division=0\n",
    "       \n",
    "        )\n",
    "       \n",
    "\n",
    "        precs.append(p); recs.append(r); f1s.append(f1)\n",
    "        accs.append(accuracy_score(y[test_idx], pred))\n",
    "        f1s.append(f1_score(y[test_idx], pred, average=\"macro\"))\n",
    "        train_pred = clf.predict(X[train_idx])\n",
    "        test_pred = clf.predict(X[test_idx])\n",
    "        train_accs.append(accuracy_score(y[train_idx], train_pred))\n",
    "        test_accs.append(accuracy_score(y[test_idx], test_pred))\n",
    "        \n",
    "    return {\n",
    "        \"size\": 3 * n_per,\n",
    "        \"acc_mean\": np.mean(accs),\n",
    "        \"f1_mean\": np.mean(f1s),\n",
    "        \"prec_mean\": np.mean(precs),\n",
    "        \"rec_mean\": np.mean(recs),\n",
    "        \"train_acc_mean\": np.mean(train_accs),\n",
    "        \"test_acc_mean\": np.mean(test_accs),\n",
    "        \"acc_gap\": np.mean(train_accs) - np.mean(test_accs)\n",
    "    }\n",
    "  \n",
    "for size in sizes:\n",
    "    print(eval_size(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf71834",
   "metadata": {},
   "source": [
    "## Rendom Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "102d9d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'size': 9999, 'acc_mean': np.float64(0.9979833333333333), 'prec_mean': np.float64(0.9979884998035753), 'rec_mean': np.float64(0.9979830905368136), 'f1_mean': np.float64(0.9979832422610725), 'train_acc_mean': np.float64(1.0), 'test_acc_mean': np.float64(0.9979833333333333), 'acc_gap': np.float64(0.0020166666666666666)}\n",
      "{'size': 19998, 'acc_mean': np.float64(0.9990500000000002), 'prec_mean': np.float64(0.9990508344130081), 'rec_mean': np.float64(0.9990500373719118), 'f1_mean': np.float64(0.9990500304744726), 'train_acc_mean': np.float64(1.0), 'test_acc_mean': np.float64(0.9990500000000002), 'acc_gap': np.float64(0.0009499999999997844)}\n",
      "{'size': 37800, 'acc_mean': np.float64(0.9994841269841268), 'prec_mean': np.float64(0.9994846543520582), 'rec_mean': np.float64(0.9994841269841268), 'f1_mean': np.float64(0.9994841381181744), 'train_acc_mean': np.float64(1.0), 'test_acc_mean': np.float64(0.9994841269841268), 'acc_gap': np.float64(0.0005158730158731917)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##random forest model \n",
    "def eval_size(total, n_splits=30):\n",
    "    n_per = min(total // 3, len(exo), len(stars), len(quasars))\n",
    "    stars_sub = stars.sample(n=n_per, random_state=42)\n",
    "    exo_sub = exo.sample(n=n_per, random_state=42)\n",
    "    quasars_sub = quasars.sample(n=n_per, random_state=42)\n",
    "\n",
    "    df = pd.concat([exo_sub, stars_sub, quasars_sub], ignore_index= True)\n",
    "    df[features] = df[features].apply(pd.to_numeric, errors='coerce')\n",
    "    df = df.dropna(subset=features)\n",
    "\n",
    "    X = df[features].values\n",
    "    y = df[\"label\"].values\n",
    "\n",
    "    splitter = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.2, random_state=42)\n",
    "    accs, f1s, precs, recs = [], [], [], []\n",
    "    train_accs, test_accs = [], []\n",
    "\n",
    "    for train_idx, test_idx in splitter.split(X, y):\n",
    "        clf = RandomForestClassifier(n_estimators=300, random_state=42, class_weight =\"balanced_subsample\")\n",
    "        clf.fit(X[train_idx], y[train_idx])\n",
    "        y_pred = clf.predict(X[test_idx])\n",
    "        p, r, f1, _ = precision_recall_fscore_support(\n",
    "            y[test_idx], y_pred, average=\"macro\", zero_division=0\n",
    "            \n",
    "        ) \n",
    "        \n",
    "        accs.append(accuracy_score(y[test_idx], y_pred))\n",
    "        precs.append(p); recs.append(r); f1s.append(f1)\n",
    "        train_pred = clf.predict(X[train_idx])\n",
    "        test_pred = clf.predict(X[test_idx])\n",
    "        train_accs.append(accuracy_score(y[train_idx], train_pred))\n",
    "        test_accs.append(accuracy_score(y[test_idx], test_pred))\n",
    "     \n",
    "\n",
    "        \n",
    "\n",
    "    return {\n",
    "        \"size\": 3 * n_per,\n",
    "        \"acc_mean\": np.mean(accs),\n",
    "        \"prec_mean\": np.mean(precs),\n",
    "        \"rec_mean\": np.mean(recs),\n",
    "        \"f1_mean\": np.mean(f1s),\n",
    "        \"train_acc_mean\": np.mean(train_accs),\n",
    "        \"test_acc_mean\": np.mean(test_accs),\n",
    "        \"acc_gap\": np.mean(train_accs) - np.mean(test_accs)\n",
    "\n",
    "    }\n",
    "        \n",
    "for size in sizes:\n",
    "    print(eval_size(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6164674e",
   "metadata": {},
   "source": [
    "## Multi-Layer Preception (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "799a6796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'size': 9999, 'acc_mean': np.float64(0.9922499999999999), 'prec_mean': np.float64(0.9922940249246686), 'rec_mean': np.float64(0.9922482202342272), 'f1_mean': np.float64(0.9922492977534353), 'train_acc_mean': np.float64(0.9929949577030462), 'test_acc_mean': np.float64(0.9922499999999999), 'acc_gap': np.float64(0.0007449577030463894)}\n",
      "{'size': 19998, 'acc_mean': np.float64(0.9956916666666665), 'prec_mean': np.float64(0.995700457733561), 'rec_mean': np.float64(0.9956918952376775), 'f1_mean': np.float64(0.9956917558814595), 'train_acc_mean': np.float64(0.9969767054215111), 'test_acc_mean': np.float64(0.9956916666666665), 'acc_gap': np.float64(0.0012850387548445363)}\n",
      "{'size': 37800, 'acc_mean': np.float64(0.9975837742504409), 'prec_mean': np.float64(0.9975882584565415), 'rec_mean': np.float64(0.9975837742504409), 'f1_mean': np.float64(0.997583762036474), 'train_acc_mean': np.float64(0.9984016754850089), 'test_acc_mean': np.float64(0.9975837742504409), 'acc_gap': np.float64(0.0008179012345680414)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "def eval_size_mlp(n_total, n_splits=30):\n",
    "    n_per = min(n_total // 3, len(stars), len(exo), len(quasars))\n",
    "    stars_sub = stars.sample(n=n_per, random_state=42)\n",
    "    exo_sub = exo.sample(n=n_per, random_state=42)\n",
    "    quasars_sub = quasars.sample(n=n_per, random_state=42)\n",
    "\n",
    "    df = pd.concat([stars_sub, exo_sub, quasars_sub], ignore_index=True)\n",
    "    # Encode labels to integers for MLP\n",
    "    le = LabelEncoder()\n",
    "    X = df[features].values\n",
    "    y = le.fit_transform(df[\"label\"].values)\n",
    "\n",
    "    splitter = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.2, random_state=42)\n",
    "    accs, precs, recs, f1s = [], [], [], []\n",
    "    train_accs, test_accs = [], []\n",
    "\n",
    "    clf = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        MLPClassifier(hidden_layer_sizes=(128, 64, 32), max_iter=1000,\n",
    "                      early_stopping=True, random_state=42)\n",
    "    )\n",
    "\n",
    "    for train_idx, test_idx in splitter.split(X, y):\n",
    "        clf.fit(X[train_idx], y[train_idx])\n",
    "        pred = clf.predict(X[test_idx])\n",
    "\n",
    "        p, r, f1, _ = precision_recall_fscore_support(\n",
    "            y[test_idx], pred, average=\"macro\", zero_division=0\n",
    "        )\n",
    "        precs.append(p); recs.append(r); f1s.append(f1)\n",
    "        accs.append(accuracy_score(y[test_idx], pred))\n",
    "\n",
    "        train_pred = clf.predict(X[train_idx])\n",
    "        train_accs.append(accuracy_score(y[train_idx], train_pred))\n",
    "        test_accs.append(accuracy_score(y[test_idx], pred))\n",
    "\n",
    "    return {\n",
    "        \"size\": 3 * n_per,\n",
    "        \"acc_mean\": np.mean(accs),\n",
    "        \"prec_mean\": np.mean(precs),\n",
    "        \"rec_mean\": np.mean(recs),\n",
    "        \"f1_mean\": np.mean(f1s),\n",
    "        \"train_acc_mean\": np.mean(train_accs),\n",
    "        \"test_acc_mean\": np.mean(test_accs),\n",
    "        \"acc_gap\": np.mean(train_accs) - np.mean(test_accs),\n",
    "    }\n",
    "\n",
    "for size in sizes:\n",
    "    print(eval_size_mlp(size))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
