{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32775a96",
   "metadata": {},
   "source": [
    "##  EXPERIMENT USING STARS, EXOPLANETS AND QUASARS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0aea39d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b10e3c",
   "metadata": {},
   "source": [
    "## checking for missing/null values (extra caution before starting the new experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "718085e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_id          0\n",
      "ra                 0\n",
      "dec                0\n",
      "parallax           0\n",
      "pmra               0\n",
      "pmdec              0\n",
      "phot_g_mean_mag    0\n",
      "bp_rp              0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   source_id        50000 non-null  int64  \n",
      " 1   ra               50000 non-null  float64\n",
      " 2   dec              50000 non-null  float64\n",
      " 3   parallax         50000 non-null  float64\n",
      " 4   pmra             50000 non-null  float64\n",
      " 5   pmdec            50000 non-null  float64\n",
      " 6   phot_g_mean_mag  50000 non-null  float64\n",
      " 7   bp_rp            50000 non-null  float64\n",
      "dtypes: float64(7), int64(1)\n",
      "memory usage: 3.1 MB\n",
      "None\n",
      "rowid              0\n",
      "pl_name            0\n",
      "hostname           0\n",
      "pl_letter          0\n",
      "gaia_id            0\n",
      "discoverymethod    0\n",
      "disc_year          0\n",
      "pl_orbper          0\n",
      "pl_orbsmax         0\n",
      "st_teff            0\n",
      "st_rad             0\n",
      "st_mass            0\n",
      "ra                 0\n",
      "dec                0\n",
      "sy_dist            0\n",
      "sy_plx             0\n",
      "pmra               0\n",
      "pmdec              0\n",
      "phot_g_mean_mag    0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37871 entries, 0 to 37870\n",
      "Data columns (total 19 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   rowid            37871 non-null  int64  \n",
      " 1   pl_name          37871 non-null  object \n",
      " 2   hostname         37871 non-null  object \n",
      " 3   pl_letter        37871 non-null  object \n",
      " 4   gaia_id          37871 non-null  object \n",
      " 5   discoverymethod  37871 non-null  object \n",
      " 6   disc_year        37871 non-null  int64  \n",
      " 7   pl_orbper        37871 non-null  float64\n",
      " 8   pl_orbsmax       37871 non-null  float64\n",
      " 9   st_teff          37871 non-null  float64\n",
      " 10  st_rad           37871 non-null  float64\n",
      " 11  st_mass          37871 non-null  float64\n",
      " 12  ra               37871 non-null  float64\n",
      " 13  dec              37871 non-null  float64\n",
      " 14  sy_dist          37871 non-null  float64\n",
      " 15  sy_plx           37871 non-null  float64\n",
      " 16  pmra             37871 non-null  float64\n",
      " 17  pmdec            37871 non-null  float64\n",
      " 18  phot_g_mean_mag  37871 non-null  float64\n",
      "dtypes: float64(12), int64(2), object(5)\n",
      "memory usage: 5.5+ MB\n",
      "None\n",
      "source_id             0\n",
      "ra                    0\n",
      "dec                   0\n",
      "parallax              0\n",
      "pmra                  0\n",
      "pmdec                 0\n",
      "phot_g_mean_mag       0\n",
      "bp_rp                 0\n",
      "ruwe                  0\n",
      "quasar_probability    0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   source_id           50000 non-null  int64  \n",
      " 1   ra                  50000 non-null  float64\n",
      " 2   dec                 50000 non-null  float64\n",
      " 3   parallax            50000 non-null  float64\n",
      " 4   pmra                50000 non-null  float64\n",
      " 5   pmdec               50000 non-null  float64\n",
      " 6   phot_g_mean_mag     50000 non-null  float64\n",
      " 7   bp_rp               50000 non-null  float64\n",
      " 8   ruwe                50000 non-null  float64\n",
      " 9   quasar_probability  50000 non-null  float64\n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 3.8 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# dealing with any missing/null values for stars, exopalnets and quasars datasets \n",
    "stars_df = pd.read_csv(\"../../data/full/stars_gaia_clean.csv\")\n",
    "print(stars_df.isnull().sum())\n",
    "print(stars_df.info())\n",
    "\n",
    "exo_df = pd.read_csv(\"../../data/full/exoplanets_clean.csv\")\n",
    "print(exo_df.isnull().sum())\n",
    "print(exo_df.info())\n",
    "\n",
    "quasars_df = pd.read_csv(\"../../data/full/quasars_gaia_clean.csv\")\n",
    "print(quasars_df.isnull().sum())\n",
    "print(quasars_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b376763",
   "metadata": {},
   "source": [
    "## Loading Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b722af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laad datasets\n",
    "stars_df = pd.read_csv(\"../../data/full/stars_gaia_clean.csv\").assign(label=\"star\")\n",
    "exo_df = pd.read_csv(\"../../data/full/exoplanets_clean.csv\").assign(label=\"exo_host\")\n",
    "quasars_df = pd.read_csv(\"../../data/full/quasars_gaia_clean.csv\").assign(label=\"quasar\")\n",
    "\n",
    "\n",
    "#align column: exo use sy_plx while stars and qiuasars use parallax\n",
    "exo_df = exo_df.rename(columns={\"sy_plx\":\"parallax\", \"sy_pmra\":\"pmra\", \"sy_pmdec\":\"pmdec\", \"sy_gaiamag\":\"phot_g_mean_mag\"})\n",
    "\n",
    "\n",
    "# select shared features\n",
    "features = [\"ra\", \"dec\", \"parallax\", \"pmdec\", \"pmra\", \"phot_g_mean_mag\"]\n",
    "\n",
    "# Define dataset size for each datasets \n",
    "sizes = [10000, 20000, 37800]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a78fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "exo_df = exo_df.dropna(subset=features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21675a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Trim datasets to only have selected features and drop rows with missing values\n",
    "stars_df = stars_df[features + [\"label\"]].dropna()\n",
    "exo_df = exo_df[features +[\"label\"]].dropna()\n",
    "quasars_df = quasars_df[features + [\"label\"]].dropna()\n",
    "\n",
    "\n",
    "\n",
    "# This satcks the three datasets toghether, so i can threat them as a single multiclass dataset\n",
    "df = pd.concat([stars_df, exo_df, quasars_df])\n",
    "\n",
    "# Split in X (Numeric Feature only) and y (Call labels (Star, Exoplanet, Quasar)\n",
    "\n",
    "X = df[features]\n",
    "y = df[\"label\"]\n",
    "\n",
    "\n",
    "# labeled-feature datasets only \n",
    "subsets = {}\n",
    "\n",
    "## Creates a balanced subsets for each dataset size\n",
    "for n_total in sizes:\n",
    "    n_per = min(n_total // 3, len(stars_df), len(exo_df), len(quasars_df))\n",
    "\n",
    "    \n",
    "    stars_sub = stars_df.sample(n=n_per, random_state=42)\n",
    "    exo_sub = exo_df.sample(n=n_per, random_state=42)\n",
    "    quasars_sub = quasars_df.sample(n=n_per, random_state=42)\n",
    "\n",
    "    df_sub = pd.concat([stars_sub, exo_sub, quasars_sub], ignore_index=True)\n",
    "    X_n = df_sub[features]\n",
    "    y_n = df_sub[\"label\"]\n",
    "    subsets[n_total] = (X_n, y_n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572342a4",
   "metadata": {},
   "source": [
    "## Baselien sanity (DummyClassifier)\n",
    "### I have 3 kinds of objects: Stars, Exoplanets, Quasars. \n",
    "### See if the model does better than random guessing, so i make a \"dummy\" model that just guesses in a simple way, and check how bad it is.\n",
    "### If tha main model beats this dummy, i know im doing real learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "824d5adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ra', 'dec', 'parallax', 'pmdec', 'pmra', 'phot_g_mean_mag', 'label'], dtype='object')\n",
      "ra                 0.0\n",
      "dec                0.0\n",
      "parallax           0.0\n",
      "pmdec              0.0\n",
      "pmra               0.0\n",
      "phot_g_mean_mag    0.0\n",
      "dtype: float64\n",
      "exo rows before dropna: 37871\n"
     ]
    }
   ],
   "source": [
    "print(exo_df.columns)\n",
    "print(exo_df[features].isna().mean().sort_values(ascending=False))\n",
    "print(\"exo rows before dropna:\", len(exo_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5adf047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy most_frequent: 0.33333333333333337 0.16666666666666669\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "X_n, y_n = subsets[10000]\n",
    "\n",
    "for n in sizes:\n",
    "    X_n, y_n = subsets[n]\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "\n",
    "accs, f1s = [], []\n",
    "for train_idx, test_idx in splitter.split(X_n, y_n):\n",
    "    clf = DummyClassifier(strategy=\"most_frequent\", random_state=42)\n",
    "    clf.fit(X_n.iloc[train_idx], y_n.iloc[train_idx])\n",
    "    pred = clf.predict(X_n.iloc[test_idx])\n",
    "    accs.append(accuracy_score(y_n.iloc[test_idx], pred))\n",
    "    f1s.append(f1_score(y_n.iloc[test_idx], pred, average=\"macro\"))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Dummy most_frequent:\", np.mean(accs), np.mean(f1s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa68ec4",
   "metadata": {},
   "source": [
    "## Logistic regression, Random Forest and Multiplayer perception (Main supervised models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9bedb052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'size': 9999, 'acc_mean': np.float64(0.9774833333333335), 'f1_mean': np.float64(0.9774779968974232)}\n",
      "{'size': 19998, 'acc_mean': np.float64(0.9794500000000002), 'f1_mean': np.float64(0.9794462269210071)}\n",
      "{'size': 37800, 'acc_mean': np.float64(0.9796869488536154), 'f1_mean': np.float64(0.9796832753779882)}\n"
     ]
    }
   ],
   "source": [
    "## Main supervised models (LogReg, RF, MPL)\n",
    "\n",
    "\n",
    "\n",
    "#align column: exo use sy_plx while stars and qiuasars use parallax\n",
    "exo_df = exo_df.rename(columns={\"sy_plx\":\"parallax\", \"sy_pmra\":\"pmra\", \"sy_pmdec\":\"pmdec\", \"sy_gaiamag\":\"phot_g_mean_mag\"})\n",
    "\n",
    "\n",
    "# select shared features\n",
    "features = [\"ra\", \"dec\", \"parallax\", \"pmdec\", \"pmra\", \"phot_g_mean_mag\"]\n",
    "\n",
    "# Define dataset size for each datasets \n",
    "sizes = [10000, 20000, 37800]\n",
    "\n",
    "# Trim datasets to only have selected features and drop rows with missing values\n",
    "stars = stars_df[features + [\"label\"]].dropna()\n",
    "exo = exo_df[features +[\"label\"]].dropna()\n",
    "quasars = quasars_df[features + [\"label\"]].dropna()\n",
    "\n",
    "def eval_size(n_total, n_splits=30):\n",
    "    n_per = min(n_total // 3, len(exo), len(stars), len(quasars))\n",
    "    stars_sub = stars.sample(n=n_per, random_state=42)\n",
    "    exo_sub = exo.sample(n=n_per, random_state=42)\n",
    "    quasars_sub = quasars.sample(n=n_per, random_state=42)\n",
    "\n",
    "    df = pd.concat([exo_sub, stars_sub, quasars_sub], ignore_index=True)\n",
    "    X = df[features].values\n",
    "    y = df[\"label\"].values\n",
    "\n",
    "    splitter = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.2, random_state=42)\n",
    "    accs, f1s = [], []\n",
    "\n",
    "    clf = make_pipeline(StandardScaler(), LogisticRegression(max_iter=2000))\n",
    "    for train_idx, test_idx in splitter.split(X, y):\n",
    "        clf.fit(X[train_idx], y[train_idx])\n",
    "        pred = clf.predict(X[test_idx])\n",
    "        accs.append(accuracy_score(y[test_idx], pred))\n",
    "        f1s.append(f1_score(y[test_idx], pred, average=\"macro\"))\n",
    "\n",
    "    return {\n",
    "        \"size\": 3 * n_per,\n",
    "        \"acc_mean\": np.mean(accs),\n",
    "        \"f1_mean\": np.mean(f1s)\n",
    "    }\n",
    "\n",
    "for size in sizes:\n",
    "    print(eval_size(size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
