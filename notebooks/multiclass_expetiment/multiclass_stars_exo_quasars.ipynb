{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32775a96",
   "metadata": {},
   "source": [
    "##  EXPERIMENT USING STARS, EXOPLANETS AND QUASARS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aea39d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b10e3c",
   "metadata": {},
   "source": [
    "## checking for missing/null values (extra caution before starting the new experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "718085e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_id          0\n",
      "ra                 0\n",
      "dec                0\n",
      "parallax           0\n",
      "pmra               0\n",
      "pmdec              0\n",
      "phot_g_mean_mag    0\n",
      "bp_rp              0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   source_id        50000 non-null  int64  \n",
      " 1   ra               50000 non-null  float64\n",
      " 2   dec              50000 non-null  float64\n",
      " 3   parallax         50000 non-null  float64\n",
      " 4   pmra             50000 non-null  float64\n",
      " 5   pmdec            50000 non-null  float64\n",
      " 6   phot_g_mean_mag  50000 non-null  float64\n",
      " 7   bp_rp            50000 non-null  float64\n",
      "dtypes: float64(7), int64(1)\n",
      "memory usage: 3.1 MB\n",
      "None\n",
      "rowid              0\n",
      "pl_name            0\n",
      "hostname           0\n",
      "pl_letter          0\n",
      "gaia_id            0\n",
      "discoverymethod    0\n",
      "disc_year          0\n",
      "pl_orbper          0\n",
      "pl_orbsmax         0\n",
      "st_teff            0\n",
      "st_rad             0\n",
      "st_mass            0\n",
      "ra                 0\n",
      "dec                0\n",
      "sy_dist            0\n",
      "sy_plx             0\n",
      "pmra               0\n",
      "pmdec              0\n",
      "phot_g_mean_mag    0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37871 entries, 0 to 37870\n",
      "Data columns (total 19 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   rowid            37871 non-null  int64  \n",
      " 1   pl_name          37871 non-null  object \n",
      " 2   hostname         37871 non-null  object \n",
      " 3   pl_letter        37871 non-null  object \n",
      " 4   gaia_id          37871 non-null  object \n",
      " 5   discoverymethod  37871 non-null  object \n",
      " 6   disc_year        37871 non-null  int64  \n",
      " 7   pl_orbper        37871 non-null  float64\n",
      " 8   pl_orbsmax       37871 non-null  float64\n",
      " 9   st_teff          37871 non-null  float64\n",
      " 10  st_rad           37871 non-null  float64\n",
      " 11  st_mass          37871 non-null  float64\n",
      " 12  ra               37871 non-null  float64\n",
      " 13  dec              37871 non-null  float64\n",
      " 14  sy_dist          37871 non-null  float64\n",
      " 15  sy_plx           37871 non-null  float64\n",
      " 16  pmra             37871 non-null  float64\n",
      " 17  pmdec            37871 non-null  float64\n",
      " 18  phot_g_mean_mag  37871 non-null  float64\n",
      "dtypes: float64(12), int64(2), object(5)\n",
      "memory usage: 5.5+ MB\n",
      "None\n",
      "source_id             0\n",
      "ra                    0\n",
      "dec                   0\n",
      "parallax              0\n",
      "pmra                  0\n",
      "pmdec                 0\n",
      "phot_g_mean_mag       0\n",
      "bp_rp                 0\n",
      "ruwe                  0\n",
      "quasar_probability    0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   source_id           50000 non-null  int64  \n",
      " 1   ra                  50000 non-null  float64\n",
      " 2   dec                 50000 non-null  float64\n",
      " 3   parallax            50000 non-null  float64\n",
      " 4   pmra                50000 non-null  float64\n",
      " 5   pmdec               50000 non-null  float64\n",
      " 6   phot_g_mean_mag     50000 non-null  float64\n",
      " 7   bp_rp               50000 non-null  float64\n",
      " 8   ruwe                50000 non-null  float64\n",
      " 9   quasar_probability  50000 non-null  float64\n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 3.8 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# dealing with any missing/null values for stars, exopalnets and quasars datasets \n",
    "stars_df = pd.read_csv(\"../../data/full/stars_gaia_clean.csv\")\n",
    "print(stars_df.isnull().sum())\n",
    "print(stars_df.info())\n",
    "\n",
    "exo_df = pd.read_csv(\"../../data/full/exoplanets_clean.csv\")\n",
    "print(exo_df.isnull().sum())\n",
    "print(exo_df.info())\n",
    "\n",
    "quasars_df = pd.read_csv(\"../../data/full/quasars_gaia_clean.csv\")\n",
    "print(quasars_df.isnull().sum())\n",
    "print(quasars_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b376763",
   "metadata": {},
   "source": [
    "## Loading Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b722af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laad datasets\n",
    "stars_df = pd.read_csv(\"../../data/full/stars_gaia_clean.csv\").assign(label=\"star\")\n",
    "exo_df = pd.read_csv(\"../../data/full/exoplanets_clean.csv\").assign(label=\"exo_host\")\n",
    "quasars_df = pd.read_csv(\"../../data/full/quasars_gaia_clean.csv\").assign(label=\"quasar\")\n",
    "\n",
    "\n",
    "#align column: exo use sy_plx while stars and qiuasars use parallax\n",
    "exo_df = exo_df.rename(columns={\"sy_plx\":\"parallax\", \"sy_pmra\":\"pmra\", \"sy_pmdec\":\"pmdec\", \"sy_gaiamag\":\"phot_g_mean_mag\"})\n",
    "\n",
    "\n",
    "# select shared features\n",
    "features = [\"ra\", \"dec\", \"parallax\", \"pmdec\", \"pmra\", \"phot_g_mean_mag\"]\n",
    "\n",
    "# Define dataset size for each datasets \n",
    "sizes = [10000, 20000, 37800]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a78fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "exo_df = exo_df.dropna(subset=features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21675a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Trim datasets to only have selected features and drop rows with missing values\n",
    "stars_df = stars_df[features + [\"label\"]].dropna()\n",
    "exo_df = exo_df[features +[\"label\"]].dropna()\n",
    "quasars_df = quasars_df[features + [\"label\"]].dropna()\n",
    "\n",
    "\n",
    "\n",
    "# This satcks the three datasets toghether, so i can threat them as a single multiclass dataset\n",
    "df = pd.concat([stars_df, exo_df, quasars_df])\n",
    "\n",
    "# Split in X (Numeric Feature only) and y (Call labels (Star, Exoplanet, Quasar)\n",
    "\n",
    "X = df[features]\n",
    "y = df[\"label\"]\n",
    "\n",
    "\n",
    "# labeled-feature datasets only \n",
    "subsets = {}\n",
    "\n",
    "## Creates a balanced subsets for each dataset size\n",
    "for n_total in sizes:\n",
    "    n_per = min(n_total // 3, len(stars_df), len(exo_df), len(quasars_df))\n",
    "\n",
    "    \n",
    "    stars_sub = stars_df.sample(n=n_per, random_state=42)\n",
    "    exo_sub = exo_df.sample(n=n_per, random_state=42)\n",
    "    quasars_sub = quasars_df.sample(n=n_per, random_state=42)\n",
    "\n",
    "    df_sub = pd.concat([stars_sub, exo_sub, quasars_sub], ignore_index=True)\n",
    "    X_n = df_sub[features]\n",
    "    y_n = df_sub[\"label\"]\n",
    "    subsets[n_total] = (X_n, y_n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572342a4",
   "metadata": {},
   "source": [
    "## Baselien sanity (DummyClassifier)\n",
    "### I have 3 kinds of objects: Stars, Exoplanets, Quasars. \n",
    "### See if the model does better than random guessing, so i make a \"dummy\" model that just guesses in a simple way, and check how bad it is.\n",
    "### If tha main model beats this dummy, i know im doing real learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "824d5adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ra', 'dec', 'parallax', 'pmdec', 'pmra', 'phot_g_mean_mag', 'label'], dtype='object')\n",
      "ra                 0.0\n",
      "dec                0.0\n",
      "parallax           0.0\n",
      "pmdec              0.0\n",
      "pmra               0.0\n",
      "phot_g_mean_mag    0.0\n",
      "dtype: float64\n",
      "exo rows before dropna: 37871\n"
     ]
    }
   ],
   "source": [
    "print(exo_df.columns)\n",
    "print(exo_df[features].isna().mean().sort_values(ascending=False))\n",
    "print(\"exo rows before dropna:\", len(exo_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5adf047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy most_frequent: 0.33333333333333337 0.16666666666666669\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "X_n, y_n = subsets[10000]\n",
    "\n",
    "for n in sizes:\n",
    "    X_n, y_n = subsets[n]\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
    "\n",
    "accs, f1s = [], []\n",
    "for train_idx, test_idx in splitter.split(X_n, y_n):\n",
    "    clf = DummyClassifier(strategy=\"most_frequent\", random_state=42)\n",
    "    clf.fit(X_n.iloc[train_idx], y_n.iloc[train_idx])\n",
    "    pred = clf.predict(X_n.iloc[test_idx])\n",
    "    accs.append(accuracy_score(y_n.iloc[test_idx], pred))\n",
    "    f1s.append(f1_score(y_n.iloc[test_idx], pred, average=\"macro\"))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Dummy most_frequent:\", np.mean(accs), np.mean(f1s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa68ec4",
   "metadata": {},
   "source": [
    "## Logistic regression, Random Forest and Multi-layer perception (Main supervised models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bedb052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'size': 9999, 'acc_mean': np.float64(0.9774833333333335), 'f1_mean': np.float64(0.9774779968974233), 'prec_mean': np.float64(0.9775730698823865), 'rec_mean': np.float64(0.9774813544178861), 'train_acc_mean': np.float64(0.9774805183981332), 'test_acc_mean': np.float64(0.9774833333333335), 'acc_gap': np.float64(-2.8149352002682093e-06)}\n",
      "{'size': 19998, 'acc_mean': np.float64(0.9794500000000002), 'f1_mean': np.float64(0.979446226921007), 'prec_mean': np.float64(0.9794854066130139), 'rec_mean': np.float64(0.9794508159773578), 'train_acc_mean': np.float64(0.9794390965537357), 'test_acc_mean': np.float64(0.9794500000000002), 'acc_gap': np.float64(-1.0903446264420857e-05)}\n",
      "{'size': 37800, 'acc_mean': np.float64(0.9796869488536154), 'f1_mean': np.float64(0.9796832753779882), 'prec_mean': np.float64(0.9797240999725578), 'rec_mean': np.float64(0.9796869488536156), 'train_acc_mean': np.float64(0.9797001763668434), 'test_acc_mean': np.float64(0.9796869488536154), 'acc_gap': np.float64(1.3227513227964671e-05)}\n"
     ]
    }
   ],
   "source": [
    "## Main supervised models (LogReg, RF, MPL)\n",
    "\n",
    "\n",
    "#align column: exo use sy_plx while stars and qiuasars use parallax\n",
    "exo_df = exo_df.rename(columns={\"sy_plx\":\"parallax\", \"sy_pmra\":\"pmra\", \"sy_pmdec\":\"pmdec\", \"sy_gaiamag\":\"phot_g_mean_mag\"})\n",
    "\n",
    "\n",
    "# select shared features\n",
    "features = [\"ra\", \"dec\", \"parallax\", \"pmdec\", \"pmra\", \"phot_g_mean_mag\"]\n",
    "\n",
    "# Define dataset size for each datasets \n",
    "sizes = [10000, 20000, 37800]\n",
    "\n",
    "# Trim datasets to only have selected features and drop rows with missing values\n",
    "stars = stars_df[features + [\"label\"]].dropna()\n",
    "exo = exo_df[features +[\"label\"]].dropna()\n",
    "quasars = quasars_df[features + [\"label\"]].dropna()\n",
    "\n",
    "def eval_size(n_total, n_splits=30):\n",
    "    n_per = min(n_total // 3, len(exo), len(stars), len(quasars))\n",
    "    stars_sub = stars.sample(n=n_per, random_state=42)\n",
    "    exo_sub = exo.sample(n=n_per, random_state=42)\n",
    "    quasars_sub = quasars.sample(n=n_per, random_state=42)\n",
    "\n",
    "    df = pd.concat([exo_sub, stars_sub, quasars_sub], ignore_index=True)\n",
    "    X = df[features].values\n",
    "    y = df[\"label\"].values\n",
    "\n",
    "    splitter = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.2, random_state=42)\n",
    "    accs, f1s, precs, recs = [], [], [], []\n",
    "    train_accs, test_accs = [], []\n",
    "\n",
    "    clf = make_pipeline(StandardScaler(), LogisticRegression(max_iter=2000))\n",
    "    for train_idx, test_idx in splitter.split(X, y):\n",
    "        clf.fit(X[train_idx], y[train_idx])\n",
    "        pred = clf.predict(X[test_idx])\n",
    "        p, r, f1, _ = precision_recall_fscore_support(\n",
    "        y[test_idx], pred, average=\"macro\", zero_division=0\n",
    "       \n",
    "        )\n",
    "       \n",
    "\n",
    "        precs.append(p); recs.append(r); f1s.append(f1)\n",
    "        accs.append(accuracy_score(y[test_idx], pred))\n",
    "        f1s.append(f1_score(y[test_idx], pred, average=\"macro\"))\n",
    "        train_pred = clf.predict(X[train_idx])\n",
    "        test_pred = clf.predict(X[test_idx])\n",
    "        train_accs.append(accuracy_score(y[train_idx], train_pred))\n",
    "        test_accs.append(accuracy_score(y[test_idx], test_pred))\n",
    "        \n",
    "    return {\n",
    "        \"size\": 3 * n_per,\n",
    "        \"acc_mean\": np.mean(accs),\n",
    "        \"f1_mean\": np.mean(f1s),\n",
    "        \"prec_mean\": np.mean(precs),\n",
    "        \"rec_mean\": np.mean(recs),\n",
    "        \"train_acc_mean\": np.mean(train_accs),\n",
    "        \"test_acc_mean\": np.mean(test_accs),\n",
    "        \"acc_gap\": np.mean(train_accs) - np.mean(test_accs)\n",
    "    }\n",
    "  \n",
    "for size in sizes:\n",
    "    print(eval_size(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf71834",
   "metadata": {},
   "source": [
    "## Rendom Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "102d9d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'size': 9999, 'acc_mean': np.float64(0.9979833333333333), 'prec_mean': np.float64(0.9979884998035753), 'rec_mean': np.float64(0.9979830905368136), 'f1_mean': np.float64(0.9979832422610725), 'train_acc_mean': np.float64(1.0), 'test_acc_mean': np.float64(0.9979833333333333), 'acc_gap': np.float64(0.0020166666666666666)}\n",
      "{'size': 19998, 'acc_mean': np.float64(0.9990500000000002), 'prec_mean': np.float64(0.9990508344130081), 'rec_mean': np.float64(0.9990500373719118), 'f1_mean': np.float64(0.9990500304744726), 'train_acc_mean': np.float64(1.0), 'test_acc_mean': np.float64(0.9990500000000002), 'acc_gap': np.float64(0.0009499999999997844)}\n",
      "{'size': 37800, 'acc_mean': np.float64(0.9994841269841268), 'prec_mean': np.float64(0.9994846543520582), 'rec_mean': np.float64(0.9994841269841268), 'f1_mean': np.float64(0.9994841381181744), 'train_acc_mean': np.float64(1.0), 'test_acc_mean': np.float64(0.9994841269841268), 'acc_gap': np.float64(0.0005158730158731917)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##random forest model \n",
    "def eval_size(total, n_splits=30):\n",
    "    n_per = min(total // 3, len(exo), len(stars), len(quasars))\n",
    "    stars_sub = stars.sample(n=n_per, random_state=42)\n",
    "    exo_sub = exo.sample(n=n_per, random_state=42)\n",
    "    quasars_sub = quasars.sample(n=n_per, random_state=42)\n",
    "\n",
    "    df = pd.concat([exo_sub, stars_sub, quasars_sub], ignore_index= True)\n",
    "    df[features] = df[features].apply(pd.to_numeric, errors='coerce')\n",
    "    df = df.dropna(subset=features)\n",
    "\n",
    "    X = df[features].values\n",
    "    y = df[\"label\"].values\n",
    "\n",
    "    splitter = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.2, random_state=42)\n",
    "    accs, f1s, precs, recs = [], [], [], []\n",
    "    train_accs, test_accs = [], []\n",
    "\n",
    "    for train_idx, test_idx in splitter.split(X, y):\n",
    "        clf = RandomForestClassifier(n_estimators=300, random_state=42, class_weight =\"balanced_subsample\")\n",
    "        clf.fit(X[train_idx], y[train_idx])\n",
    "        y_pred = clf.predict(X[test_idx])\n",
    "        p, r, f1, _ = precision_recall_fscore_support(\n",
    "            y[test_idx], y_pred, average=\"macro\", zero_division=0\n",
    "            \n",
    "        ) \n",
    "        \n",
    "        accs.append(accuracy_score(y[test_idx], y_pred))\n",
    "        precs.append(p); recs.append(r); f1s.append(f1)\n",
    "        train_pred = clf.predict(X[train_idx])\n",
    "        test_pred = clf.predict(X[test_idx])\n",
    "        train_accs.append(accuracy_score(y[train_idx], train_pred))\n",
    "        test_accs.append(accuracy_score(y[test_idx], test_pred))\n",
    "     \n",
    "\n",
    "        \n",
    "\n",
    "    return {\n",
    "        \"size\": 3 * n_per,\n",
    "        \"acc_mean\": np.mean(accs),\n",
    "        \"prec_mean\": np.mean(precs),\n",
    "        \"rec_mean\": np.mean(recs),\n",
    "        \"f1_mean\": np.mean(f1s),\n",
    "        \"train_acc_mean\": np.mean(train_accs),\n",
    "        \"test_acc_mean\": np.mean(test_accs),\n",
    "        \"acc_gap\": np.mean(train_accs) - np.mean(test_accs)\n",
    "\n",
    "    }\n",
    "        \n",
    "for size in sizes:\n",
    "    print(eval_size(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6164674e",
   "metadata": {},
   "source": [
    "## Multi-Layer Preception (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799a6796",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[2. 1. 0. ... 0. 1. 1.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     46\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msize\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3\u001b[39m * n_per,\n\u001b[32m     47\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33macc_mean\u001b[39m\u001b[33m\"\u001b[39m: np.mean(accs),\n\u001b[32m   (...)\u001b[39m\u001b[32m     53\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33macc_gap\u001b[39m\u001b[33m\"\u001b[39m: np.mean(train_accs) - np.mean(test_accs),\n\u001b[32m     54\u001b[39m     }\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m size \u001b[38;5;129;01min\u001b[39;00m sizes:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[43meval_size_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36meval_size_mlp\u001b[39m\u001b[34m(n_total, n_splits)\u001b[39m\n\u001b[32m     25\u001b[39m clf = make_pipeline(\n\u001b[32m     26\u001b[39m     StandardScaler(),\n\u001b[32m     27\u001b[39m     MLPClassifier(hidden_layer_sizes=(\u001b[32m128\u001b[39m, \u001b[32m64\u001b[39m, \u001b[32m32\u001b[39m), max_iter=\u001b[32m1000\u001b[39m,\n\u001b[32m     28\u001b[39m                   early_stopping=\u001b[38;5;28;01mTrue\u001b[39;00m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     29\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m train_idx, test_idx \u001b[38;5;129;01min\u001b[39;00m splitter.split(X, y):\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     pred = clf.predict(X[test_idx])\n\u001b[32m     35\u001b[39m     p, r, f1, _ = precision_recall_fscore_support(\n\u001b[32m     36\u001b[39m         y[test_idx], pred, average=\u001b[33m\"\u001b[39m\u001b[33mmacro\u001b[39m\u001b[33m\"\u001b[39m, zero_division=\u001b[32m0\u001b[39m\n\u001b[32m     37\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferra\\Desktop\\Celestial-Object-Identification\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferra\\Desktop\\Celestial-Object-Identification\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:655\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    649\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    650\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    651\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    652\u001b[39m     )\n\u001b[32m    654\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._check_method_params(method=\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, props=params)\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m Xt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps) - \u001b[32m1\u001b[39m)):\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferra\\Desktop\\Celestial-Object-Identification\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:589\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, X, y, routed_params, raw_params)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[32m    583\u001b[39m step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    584\u001b[39m     step_idx=step_idx,\n\u001b[32m    585\u001b[39m     step_params=routed_params[name],\n\u001b[32m    586\u001b[39m     all_params=raw_params,\n\u001b[32m    587\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m X, fitted_transformer = \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPipeline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    598\u001b[39m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[32m    600\u001b[39m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[32m    601\u001b[39m \u001b[38;5;28mself\u001b[39m.steps[step_idx] = (name, fitted_transformer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferra\\Desktop\\Celestial-Object-Identification\\.venv\\Lib\\site-packages\\joblib\\memory.py:326\u001b[39m, in \u001b[36mNotMemorizedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferra\\Desktop\\Celestial-Object-Identification\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:1540\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1539\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m         res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1541\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1542\u001b[39m         res = transformer.fit(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, {})).transform(\n\u001b[32m   1543\u001b[39m             X, **params.get(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1544\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferra\\Desktop\\Celestial-Object-Identification\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferra\\Desktop\\Celestial-Object-Identification\\.venv\\Lib\\site-packages\\sklearn\\base.py:897\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    894\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, **fit_params).transform(X)\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    896\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferra\\Desktop\\Celestial-Object-Identification\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:907\u001b[39m, in \u001b[36mStandardScaler.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;28mself\u001b[39m._reset()\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferra\\Desktop\\Celestial-Object-Identification\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferra\\Desktop\\Celestial-Object-Identification\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:943\u001b[39m, in \u001b[36mStandardScaler.partial_fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    911\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[32m    912\u001b[39m \n\u001b[32m    913\u001b[39m \u001b[33;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m \u001b[33;03m    Fitted scaler.\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    942\u001b[39m first_call = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn_samples_seen_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m n_features = X.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferra\\Desktop\\Celestial-Object-Identification\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ferra\\Desktop\\Celestial-Object-Identification\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1091\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1084\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1085\u001b[39m             msg = (\n\u001b[32m   1086\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1087\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1088\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1089\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mif it contains a single sample.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1090\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array.dtype, \u001b[33m\"\u001b[39m\u001b[33mkind\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array.dtype.kind \u001b[38;5;129;01min\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mUSV\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1094\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1095\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdtype=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mnumeric\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1096\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1097\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Expected 2D array, got 1D array instead:\narray=[2. 1. 0. ... 0. 1. 1.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "def eval_size_mlp(n_total, n_splits=30):\n",
    "    n_per = min(n_total // 3, len(stars), len(exo), len(quasars))\n",
    "    stars_sub = stars.sample(n=n_per, random_state=42)\n",
    "    exo_sub = exo.sample(n=n_per, random_state=42)\n",
    "    quasars_sub = quasars.sample(n=n_per, random_state=42)\n",
    "\n",
    "    df = pd.concat([stars_sub, exo_sub, quasars_sub], ignore_index=True)\n",
    "    # Encode labels to integers for MLP\n",
    "    le = LabelEncoder()\n",
    "    X = df[features].values\n",
    "    y = le.fit_transform(df[\"label\"].values)\n",
    "\n",
    "    splitter = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.2, random_state=42)\n",
    "    accs, precs, recs, f1s = [], [], [], []\n",
    "    train_accs, test_accs = [], []\n",
    "\n",
    "    clf = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        MLPClassifier(hidden_layer_sizes=(128, 64, 32), max_iter=1000,\n",
    "                      early_stopping=True, random_state=42)\n",
    "    )\n",
    "\n",
    "    for train_idx, test_idx in splitter.split(X, y):\n",
    "        clf.fit(X[train_idx], y[train_idx])\n",
    "        pred = clf.predict(X[test_idx])\n",
    "\n",
    "        p, r, f1, _ = precision_recall_fscore_support(\n",
    "            y[test_idx], pred, average=\"macro\", zero_division=0\n",
    "        )\n",
    "        precs.append(p); recs.append(r); f1s.append(f1)\n",
    "        accs.append(accuracy_score(y[test_idx], pred))\n",
    "\n",
    "        train_pred = clf.predict(X[train_idx])\n",
    "        train_accs.append(accuracy_score(y[train_idx], train_pred))\n",
    "        test_accs.append(accuracy_score(y[test_idx], pred))\n",
    "\n",
    "    return {\n",
    "        \"size\": 3 * n_per,\n",
    "        \"acc_mean\": np.mean(accs),\n",
    "        \"prec_mean\": np.mean(precs),\n",
    "        \"rec_mean\": np.mean(recs),\n",
    "        \"f1_mean\": np.mean(f1s),\n",
    "        \"train_acc_mean\": np.mean(train_accs),\n",
    "        \"test_acc_mean\": np.mean(test_accs),\n",
    "        \"acc_gap\": np.mean(train_accs) - np.mean(test_accs),\n",
    "    }\n",
    "\n",
    "for size in sizes:\n",
    "    print(eval_size_mlp(size))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
